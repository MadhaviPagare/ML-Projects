{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2\n",
    "  \n",
    "  * CSCI-5930 ML Fall 2021  \n",
    "  * Author: Madhavi S Pagare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# First load the dataset into pandas dataframe\n",
    "full_dataset = pd.read_csv('dataset/baby-weights-dataset.csv',delimiter=',')\n",
    "judge_dataset = pd.read_csv('dataset/judge-without-labels.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks for everyone (Tasks 1-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: \n",
    "Separate the full_dataset into two parts: X and y, where X denotes the input matrix containing only the input (i.e., independent explanatory) variables, and y denotes the target variable containing only the target values for exactly the same number of samples in the given full_dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  \\\n",
      "0         2001    2        1    33    26.0      10    34   12.0      4   \n",
      "1         2002    2        2    19    40.0      10    18   11.0     12   \n",
      "2         2003    2        1    33    16.0      14    31   16.0     16   \n",
      "3         2004    1        1    25    40.0      15    28   12.0     12   \n",
      "4         2005    1        2    21    60.0      13    20   12.0     14   \n",
      "...        ...  ...      ...   ...     ...     ...   ...    ...    ...   \n",
      "101395  103396    1        2    36     0.0       9    34    3.0     12   \n",
      "101396  103397    2        2    21    39.0      11    19   12.0      9   \n",
      "101397  103398    2        1    27    37.0      15    22   12.0     12   \n",
      "101398  103399    1        1    27    26.0      12    24   12.0     14   \n",
      "101399  103400    1        2    20    31.0      15    17   12.0     11   \n",
      "\n",
      "        TOTALP  ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX  PINFANT  \\\n",
      "0            2  ...         0        0        0       0       0        0   \n",
      "1            1  ...         0        0        0       0       0        0   \n",
      "2            2  ...         0        0        0       0       0        0   \n",
      "3            3  ...         0        0        0       0       0        0   \n",
      "4            2  ...         0        0        1       0       0        0   \n",
      "...        ...  ...       ...      ...      ...     ...     ...      ...   \n",
      "101395       4  ...         0        0        0       0       0        0   \n",
      "101396       2  ...         0        0        0       0       0        0   \n",
      "101397       2  ...         0        0        0       0       0        0   \n",
      "101398       1  ...         0        0        0       0       0        0   \n",
      "101399       1  ...         0        0        0       0       0        0   \n",
      "\n",
      "       PRETERM RENAL  RHSEN  UTERINE  \n",
      "0            0     0      0        0  \n",
      "1            0     0      0        0  \n",
      "2            0     0      0        0  \n",
      "3            0     0      0        0  \n",
      "4            0     0      0        0  \n",
      "...        ...   ...    ...      ...  \n",
      "101395       0     0      0        0  \n",
      "101396       0     0      0        0  \n",
      "101397       0     0      0        0  \n",
      "101398       0     0      0        0  \n",
      "101399       0     0      0        0  \n",
      "\n",
      "[101400 rows x 36 columns]\n",
      "0         4.3750\n",
      "1         6.9375\n",
      "2         8.5000\n",
      "3         8.5000\n",
      "4         9.0000\n",
      "           ...  \n",
      "101395    9.1250\n",
      "101396    7.3750\n",
      "101397    7.5000\n",
      "101398    7.6250\n",
      "101399    6.2500\n",
      "Name: BWEIGHT, Length: 101400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "#@So below is the X denoting independent explanatory variables \n",
    "X = full_dataset.drop('BWEIGHT', axis=1)\n",
    "#@ And y denoting the target variable\n",
    "y = full_dataset['BWEIGHT']\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2:\n",
    "* Given X representing the input matrix from the full_dataset, y being the target vector (the rightmost column of the full_dataset), obtained from Task 1: \n",
    "* randomly split the (X,y) dataset into 75% for training and 25% for testing using the library function from the library [sklearn.model_selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) . Please pass to the train_test_split function an additional argument random_state=45931.\n",
    "* Store the 4 splits as X_train, X_test, y_train, y_test respectively.\n",
    "* Save the ID column for X_train and X_test into ID_train and ID_test as list variable.\n",
    "* Now, drop the ID columns from both X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  \\\n",
      "16961  18962    1        1    36    50.0      16    32   12.0     13       3   \n",
      "28131  30132    2        2    23    50.0      13    20   12.0     15       2   \n",
      "35114  37115    1        1    26    38.0      11    21   12.0     13       1   \n",
      "85941  87942    1        2    22    20.0      10    19   12.0     12       1   \n",
      "26819  28820    2        1    35    30.0      13    30   16.0     17       3   \n",
      "...      ...  ...      ...   ...     ...     ...   ...    ...    ...     ...   \n",
      "62226  64227    1        2    19    51.0      18    18   12.0     12       1   \n",
      "87904  89905    2        2    42    38.0       4    27    9.0     11       5   \n",
      "32338  34339    1        1    37    30.0      10    31   14.0     14       2   \n",
      "27127  29128    1        1    23    23.0      13    37   12.0     14       2   \n",
      "28789  30790    2        1    31    24.0      10    31   17.0     16       2   \n",
      "\n",
      "       ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX  PINFANT PRETERM RENAL  \\\n",
      "16961  ...         0        0        0       0       0        1       0     0   \n",
      "28131  ...         0        0        0       0       0        0       0     0   \n",
      "35114  ...         0        0        1       0       0        0       0     0   \n",
      "85941  ...         0        0        0       0       0        0       0     0   \n",
      "26819  ...         0        0        0       0       0        0       0     0   \n",
      "...    ...       ...      ...      ...     ...     ...      ...     ...   ...   \n",
      "62226  ...         0        0        0       0       0        0       0     0   \n",
      "87904  ...         0        0        0       0       0        0       0     0   \n",
      "32338  ...         0        0        0       0       0        0       0     0   \n",
      "27127  ...         0        0        0       0       0        0       0     0   \n",
      "28789  ...         0        0        0       0       0        0       0     0   \n",
      "\n",
      "       RHSEN  UTERINE  \n",
      "16961      0        0  \n",
      "28131      0        0  \n",
      "35114      0        0  \n",
      "85941      0        0  \n",
      "26819      0        0  \n",
      "...      ...      ...  \n",
      "62226      0        0  \n",
      "87904      0        0  \n",
      "32338      0        0  \n",
      "27127      0        0  \n",
      "28789      0        0  \n",
      "\n",
      "[76050 rows x 36 columns]             ID  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  \\\n",
      "40925    42926    2        1    30    33.0      16    29   16.0     17   \n",
      "46370    48371    2        1    23    20.0      10    22   12.0     15   \n",
      "78590    80591    1        2    24    34.0      12    21   10.0     14   \n",
      "101381  103382    2        1    32    16.0      15    28   12.0     13   \n",
      "43749    45750    2        1    26    30.0       7    30   16.0     16   \n",
      "...        ...  ...      ...   ...     ...     ...   ...    ...    ...   \n",
      "88421    90422    2        1    26     0.0       8    27   13.0     14   \n",
      "14930    16931    2        1    33    30.0      12    29    8.0      6   \n",
      "26450    28451    2        1    31    28.0       3    34   10.0     10   \n",
      "10257    12258    1        1    38    29.0      10    39   17.0     17   \n",
      "85343    87344    1        1    37    30.0      12    37   16.0     17   \n",
      "\n",
      "        TOTALP  ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX  PINFANT  \\\n",
      "40925        1  ...         0        0        0       0       0        0   \n",
      "46370        2  ...         0        1        0       0       0        0   \n",
      "78590        2  ...         0        0        0       0       0        0   \n",
      "101381       2  ...         0        0        0       0       0        0   \n",
      "43749        4  ...         0        0        0       0       0        0   \n",
      "...        ...  ...       ...      ...      ...     ...     ...      ...   \n",
      "88421        2  ...         0        0        0       0       0        0   \n",
      "14930        4  ...         0        0        0       0       0        0   \n",
      "26450        4  ...         0        0        0       0       0        0   \n",
      "10257        2  ...         0        0        0       0       0        0   \n",
      "85343        3  ...         0        0        0       0       0        0   \n",
      "\n",
      "       PRETERM RENAL  RHSEN  UTERINE  \n",
      "40925        0     0      0        0  \n",
      "46370        0     0      0        0  \n",
      "78590        0     0      0        0  \n",
      "101381       0     0      0        0  \n",
      "43749        0     0      0        0  \n",
      "...        ...   ...    ...      ...  \n",
      "88421        0     0      0        0  \n",
      "14930        0     0      0        0  \n",
      "26450        0     0      0        0  \n",
      "10257        0     0      0        0  \n",
      "85343        0     0      0        0  \n",
      "\n",
      "[25350 rows x 36 columns] 16961    9.0000\n",
      "28131    9.0625\n",
      "35114    7.1250\n",
      "85941    6.8125\n",
      "26819    7.1875\n",
      "          ...  \n",
      "62226    8.6250\n",
      "87904    8.5625\n",
      "32338    8.4375\n",
      "27127    8.0625\n",
      "28789    8.4375\n",
      "Name: BWEIGHT, Length: 76050, dtype: float64 40925     7.1875\n",
      "46370     5.3125\n",
      "78590     9.2500\n",
      "101381    8.1875\n",
      "43749     7.6250\n",
      "           ...  \n",
      "88421     7.1875\n",
      "14930     8.8125\n",
      "26450     7.8750\n",
      "10257     6.3125\n",
      "85343     8.3750\n",
      "Name: BWEIGHT, Length: 25350, dtype: float64\n",
      "       SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  BDEAD  \\\n",
      "16961    1        1    36    50.0      16    32   12.0     13       3      0   \n",
      "28131    2        2    23    50.0      13    20   12.0     15       2      0   \n",
      "35114    1        1    26    38.0      11    21   12.0     13       1      0   \n",
      "85941    1        2    22    20.0      10    19   12.0     12       1      0   \n",
      "26819    2        1    35    30.0      13    30   16.0     17       3      0   \n",
      "...    ...      ...   ...     ...     ...   ...    ...    ...     ...    ...   \n",
      "62226    1        2    19    51.0      18    18   12.0     12       1      0   \n",
      "87904    2        2    42    38.0       4    27    9.0     11       5      0   \n",
      "32338    1        1    37    30.0      10    31   14.0     14       2      0   \n",
      "27127    1        1    23    23.0      13    37   12.0     14       2      0   \n",
      "28789    2        1    31    24.0      10    31   17.0     16       2      0   \n",
      "\n",
      "       ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX PINFANT PRETERM  RENAL  \\\n",
      "16961  ...         0        0        0       0       0       1       0      0   \n",
      "28131  ...         0        0        0       0       0       0       0      0   \n",
      "35114  ...         0        0        1       0       0       0       0      0   \n",
      "85941  ...         0        0        0       0       0       0       0      0   \n",
      "26819  ...         0        0        0       0       0       0       0      0   \n",
      "...    ...       ...      ...      ...     ...     ...     ...     ...    ...   \n",
      "62226  ...         0        0        0       0       0       0       0      0   \n",
      "87904  ...         0        0        0       0       0       0       0      0   \n",
      "32338  ...         0        0        0       0       0       0       0      0   \n",
      "27127  ...         0        0        0       0       0       0       0      0   \n",
      "28789  ...         0        0        0       0       0       0       0      0   \n",
      "\n",
      "       RHSEN  UTERINE  \n",
      "16961      0        0  \n",
      "28131      0        0  \n",
      "35114      0        0  \n",
      "85941      0        0  \n",
      "26819      0        0  \n",
      "...      ...      ...  \n",
      "62226      0        0  \n",
      "87904      0        0  \n",
      "32338      0        0  \n",
      "27127      0        0  \n",
      "28789      0        0  \n",
      "\n",
      "[76050 rows x 35 columns]\n",
      "        SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  BDEAD  \\\n",
      "40925     2        1    30    33.0      16    29   16.0     17       1      0   \n",
      "46370     2        1    23    20.0      10    22   12.0     15       2      0   \n",
      "78590     1        2    24    34.0      12    21   10.0     14       2      0   \n",
      "101381    2        1    32    16.0      15    28   12.0     13       2      0   \n",
      "43749     2        1    26    30.0       7    30   16.0     16       4      0   \n",
      "...     ...      ...   ...     ...     ...   ...    ...    ...     ...    ...   \n",
      "88421     2        1    26     0.0       8    27   13.0     14       2      0   \n",
      "14930     2        1    33    30.0      12    29    8.0      6       4      0   \n",
      "26450     2        1    31    28.0       3    34   10.0     10       4      0   \n",
      "10257     1        1    38    29.0      10    39   17.0     17       2      0   \n",
      "85343     1        1    37    30.0      12    37   16.0     17       3      0   \n",
      "\n",
      "        ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX PINFANT PRETERM  \\\n",
      "40925   ...         0        0        0       0       0       0       0   \n",
      "46370   ...         0        1        0       0       0       0       0   \n",
      "78590   ...         0        0        0       0       0       0       0   \n",
      "101381  ...         0        0        0       0       0       0       0   \n",
      "43749   ...         0        0        0       0       0       0       0   \n",
      "...     ...       ...      ...      ...     ...     ...     ...     ...   \n",
      "88421   ...         0        0        0       0       0       0       0   \n",
      "14930   ...         0        0        0       0       0       0       0   \n",
      "26450   ...         0        0        0       0       0       0       0   \n",
      "10257   ...         0        0        0       0       0       0       0   \n",
      "85343   ...         0        0        0       0       0       0       0   \n",
      "\n",
      "        RENAL  RHSEN  UTERINE  \n",
      "40925       0      0        0  \n",
      "46370       0      0        0  \n",
      "78590       0      0        0  \n",
      "101381      0      0        0  \n",
      "43749       0      0        0  \n",
      "...       ...    ...      ...  \n",
      "88421       0      0        0  \n",
      "14930       0      0        0  \n",
      "26450       0      0        0  \n",
      "10257       0      0        0  \n",
      "85343       0      0        0  \n",
      "\n",
      "[25350 rows x 35 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MADHAVI\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#@below is the train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75,random_state=45931)\n",
    "#@storing the 4 splits as X_train,X_test,y_train,y_test\n",
    "print(X_train,X_test,y_train,y_test)\n",
    "#@saving below is the ID column for X_train and X_test into ID_train and ID_test  as list variable\n",
    "ID_train=X_train['ID']\n",
    "ID_test=X_test['ID']\n",
    "#@print(ID_train) \n",
    "#@print(ID_test)\n",
    "#@Dropping the ID columns from both X_train and X_test\n",
    "X_train.drop(['ID'],axis=1, inplace=True)\n",
    "X_test.drop(['ID'],axis=1, inplace=True)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3:\n",
    "Compute mean, stdev, min, max, 25% percentile, median and 75% percentile of BWEIGHT target variable (i.e, the target y) in the training set (i.e., y_train), and print the computed values as a numpy array containing these 7 results (respectively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    76050.000000\n",
      "mean         7.256998\n",
      "std          1.330058\n",
      "min          0.312500\n",
      "25%          6.625000\n",
      "50%          7.375000\n",
      "75%          8.062500\n",
      "max         13.062500\n",
      "Name: BWEIGHT, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.25699786,  1.33005843,  0.3125    , 13.0625    ,  6.625     ,\n",
       "        7.375     ,  8.0625    ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#@TODO: Your code goes here\n",
    "#@describe function is used for computing a summary of stats with regards to dataframe columns for y_train\n",
    "print(y_train.describe())\n",
    "\n",
    "#@ Mean=np.mean(BWEIGHT)\n",
    "#@ stdev= data[BWEIGHT].std()\n",
    "#@ min=np.min(BWEIGHT)\n",
    "#@ max=np.max(BWEIGHT)\n",
    "#@ percentile=np.percentile(BWEIGHT, 25)\n",
    "#@ median=np.median(BWEIGHT)\n",
    "#@ BWEIGHT=np.percentile(data[BWEIGHT],75)\n",
    "#@ result = result.append(pd.Series([mean,stdev,min,max,percentile,median,BWEIGHT],index=result.columns),ignore_index=True)\n",
    "\n",
    "#@NOTE: I am using y_train variable for  calculating the above required parameters-mean,stdev,min,max,25% percentile,median and 75% percentile Of BWEIGHT target variable'''\n",
    "y_train_info = np.array([y_train.mean(), y_train.std(), y_train.min(), y_train.max(), \\\n",
    "                         np.percentile(y_train, 25), y_train.median(), np.percentile(y_train, 75)])\n",
    "#@info function is used for printing concise summary of y_train dataframe cols\n",
    "y_train_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: \n",
    "Given the training dataset (X_train, y_train), save as X_train_ohe after replacing all the non-numeric variables (i.e., categorical variables) with numeric encoding. Please consider using the \"One-hot encoding\" scheme i.e., introducing dummy variables. A brief description of the scheme can be found in the [DUMMY-variables.note.txt](DUMMY-variables.note.txt) file\n",
    "* Use the same encoder to perform onehotencoding on the X_test dataset and save the result as X_test_ohe.\n",
    "* Print the column names of X_test_ohe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical boolean mask\n",
    "categorical_feature_mask = X_train.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = X_train.columns[categorical_feature_mask].tolist()\n",
    "cat_columns_idx = [X_train.columns.get_loc(col) \n",
    "                   for col in categorical_cols]\n",
    "#@print(cat_columns_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use when different features need different preprocessing\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(sparse = False), categorical_cols)\n",
    ")\n",
    "#@print(column_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  \\\n",
      "0      16961    1        1    36    50.0      16    32   12.0     13       3   \n",
      "1      28131    2        2    23    50.0      13    20   12.0     15       2   \n",
      "2      35114    1        1    26    38.0      11    21   12.0     13       1   \n",
      "3      85941    1        2    22    20.0      10    19   12.0     12       1   \n",
      "4      26819    2        1    35    30.0      13    30   16.0     17       3   \n",
      "...      ...  ...      ...   ...     ...     ...   ...    ...    ...     ...   \n",
      "76045  62226    1        2    19    51.0      18    18   12.0     12       1   \n",
      "76046  87904    2        2    42    38.0       4    27    9.0     11       5   \n",
      "76047  32338    1        1    37    30.0      10    31   14.0     14       2   \n",
      "76048  27127    1        1    23    23.0      13    37   12.0     14       2   \n",
      "76049  28789    2        1    31    24.0      10    31   17.0     16       2   \n",
      "\n",
      "       ...  onehotencoder__x0_P  onehotencoder__x0_S  onehotencoder__x0_U  \\\n",
      "0      ...                  0.0                  0.0                  0.0   \n",
      "1      ...                  0.0                  0.0                  0.0   \n",
      "2      ...                  0.0                  0.0                  0.0   \n",
      "3      ...                  0.0                  0.0                  0.0   \n",
      "4      ...                  0.0                  0.0                  0.0   \n",
      "...    ...                  ...                  ...                  ...   \n",
      "76045  ...                  0.0                  0.0                  0.0   \n",
      "76046  ...                  0.0                  0.0                  0.0   \n",
      "76047  ...                  0.0                  0.0                  0.0   \n",
      "76048  ...                  0.0                  0.0                  0.0   \n",
      "76049  ...                  0.0                  0.0                  0.0   \n",
      "\n",
      "       onehotencoder__x1_C  onehotencoder__x1_M  onehotencoder__x1_N  \\\n",
      "0                      0.0                  0.0                  1.0   \n",
      "1                      0.0                  0.0                  1.0   \n",
      "2                      0.0                  0.0                  1.0   \n",
      "3                      0.0                  0.0                  1.0   \n",
      "4                      0.0                  0.0                  1.0   \n",
      "...                    ...                  ...                  ...   \n",
      "76045                  0.0                  0.0                  1.0   \n",
      "76046                  0.0                  0.0                  1.0   \n",
      "76047                  0.0                  0.0                  1.0   \n",
      "76048                  0.0                  0.0                  1.0   \n",
      "76049                  0.0                  0.0                  1.0   \n",
      "\n",
      "       onehotencoder__x1_O  onehotencoder__x1_P  onehotencoder__x1_S  \\\n",
      "0                      0.0                  0.0                  0.0   \n",
      "1                      0.0                  0.0                  0.0   \n",
      "2                      0.0                  0.0                  0.0   \n",
      "3                      0.0                  0.0                  0.0   \n",
      "4                      0.0                  0.0                  0.0   \n",
      "...                    ...                  ...                  ...   \n",
      "76045                  0.0                  0.0                  0.0   \n",
      "76046                  0.0                  0.0                  0.0   \n",
      "76047                  0.0                  0.0                  0.0   \n",
      "76048                  0.0                  0.0                  0.0   \n",
      "76049                  0.0                  0.0                  0.0   \n",
      "\n",
      "       onehotencoder__x1_U  \n",
      "0                      0.0  \n",
      "1                      0.0  \n",
      "2                      0.0  \n",
      "3                      0.0  \n",
      "4                      0.0  \n",
      "...                    ...  \n",
      "76045                  0.0  \n",
      "76046                  0.0  \n",
      "76047                  0.0  \n",
      "76048                  0.0  \n",
      "76049                  0.0  \n",
      "\n",
      "[76050 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "#@performed one hot encoding on the X_train dataset and saving results in X_train_ohe\n",
    "X_train_ohe = X_train.drop(categorical_cols, axis= 1).reset_index()\n",
    "'''print(X_train_ohe)'''\n",
    "y_train.reset_index(inplace=True,drop=True)\n",
    "X_train_ohe1 = pd.DataFrame(column_trans.fit_transform(X_train),columns=column_trans.get_feature_names())\n",
    "'''print(X_train_ohe1)'''\n",
    "X_train_ohe = pd.concat([X_train_ohe,X_train_ohe1], axis=1)\n",
    "print(X_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Using the same encoder for performing one hot encoding on the X_test dataset and saving results in X_test_ohe\n",
    "X_test_ohe = X_test.drop(categorical_cols, axis= 1).reset_index()\n",
    "#@print(X_test_ohe)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "X_test_ohe1 = pd.DataFrame(column_trans.fit_transform(X_test),columns=column_trans.get_feature_names())\n",
    "#@print(X_test_ohe1)\n",
    "X_test_ohe = pd.concat([X_test_ohe, X_test_ohe1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'SEX', 'MARITAL', 'FAGE', 'GAINED', 'VISITS', 'MAGE', 'FEDUC',\n",
      "       'MEDUC', 'TOTALP', 'BDEAD', 'TERMS', 'LOUTCOME', 'WEEKS', 'RACEMOM',\n",
      "       'RACEDAD', 'CIGNUM', 'DRINKNUM', 'ANEMIA', 'CARDIAC', 'ACLUNG',\n",
      "       'DIABETES', 'HERPES', 'HYDRAM', 'HEMOGLOB', 'HYPERCH', 'HYPERPR',\n",
      "       'ECLAMP', 'CERVIX', 'PINFANT', 'PRETERM', 'RENAL', 'RHSEN', 'UTERINE',\n",
      "       'onehotencoder__x0_C', 'onehotencoder__x0_M', 'onehotencoder__x0_N',\n",
      "       'onehotencoder__x0_O', 'onehotencoder__x0_P', 'onehotencoder__x0_S',\n",
      "       'onehotencoder__x0_U', 'onehotencoder__x1_C', 'onehotencoder__x1_M',\n",
      "       'onehotencoder__x1_N', 'onehotencoder__x1_O', 'onehotencoder__x1_P',\n",
      "       'onehotencoder__x1_S', 'onehotencoder__x1_U'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@TODO: Your code goes here\n",
    "print(X_test_ohe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: \n",
    "* Given the X_train_ohe (Onehot encoded Pandas Dataframe from Task 4), check if there are missing values, and if yes, count how many, and impute the missing values with corresponding mean values. \n",
    "* Finally, print the counting result as a Pandas dataframe named \"missing_counts\" having 2 columns {variable_name,num_of_missing_values).  Please make sure that the result lists all the input variables in the given dataset. \n",
    "* Now, impute the missing values by mean of the respective variable and save the revised dataframe as X_train_ohe_imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          variable_name  num_of_missing_values\n",
      "0                 index                      0\n",
      "1                   SEX                      0\n",
      "2               MARITAL                      0\n",
      "3                  FAGE                      0\n",
      "4                GAINED                      1\n",
      "5                VISITS                      0\n",
      "6                  MAGE                      0\n",
      "7                 FEDUC                      1\n",
      "8                 MEDUC                      0\n",
      "9                TOTALP                      0\n",
      "10                BDEAD                      0\n",
      "11                TERMS                      0\n",
      "12             LOUTCOME                      0\n",
      "13                WEEKS                      1\n",
      "14              RACEMOM                      0\n",
      "15              RACEDAD                      0\n",
      "16               CIGNUM                      1\n",
      "17             DRINKNUM                      0\n",
      "18               ANEMIA                      0\n",
      "19              CARDIAC                      0\n",
      "20               ACLUNG                      0\n",
      "21             DIABETES                      0\n",
      "22               HERPES                      0\n",
      "23               HYDRAM                      1\n",
      "24             HEMOGLOB                      0\n",
      "25              HYPERCH                      0\n",
      "26              HYPERPR                      0\n",
      "27               ECLAMP                      0\n",
      "28               CERVIX                      0\n",
      "29              PINFANT                      0\n",
      "30              PRETERM                      0\n",
      "31                RENAL                      0\n",
      "32                RHSEN                      0\n",
      "33              UTERINE                      0\n",
      "34  onehotencoder__x0_C                      0\n",
      "35  onehotencoder__x0_M                      0\n",
      "36  onehotencoder__x0_N                      0\n",
      "37  onehotencoder__x0_O                      0\n",
      "38  onehotencoder__x0_P                      0\n",
      "39  onehotencoder__x0_S                      0\n",
      "40  onehotencoder__x0_U                      0\n",
      "41  onehotencoder__x1_C                      0\n",
      "42  onehotencoder__x1_M                      0\n",
      "43  onehotencoder__x1_N                      0\n",
      "44  onehotencoder__x1_O                      0\n",
      "45  onehotencoder__x1_P                      0\n",
      "46  onehotencoder__x1_S                      0\n",
      "47  onehotencoder__x1_U                      0\n",
      "       index  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  \\\n",
      "0      16961    1        1    36    50.0      16    32   12.0     13       3   \n",
      "1      28131    2        2    23    50.0      13    20   12.0     15       2   \n",
      "2      35114    1        1    26    38.0      11    21   12.0     13       1   \n",
      "3      85941    1        2    22    20.0      10    19   12.0     12       1   \n",
      "4      26819    2        1    35    30.0      13    30   16.0     17       3   \n",
      "...      ...  ...      ...   ...     ...     ...   ...    ...    ...     ...   \n",
      "76045  62226    1        2    19    51.0      18    18   12.0     12       1   \n",
      "76046  87904    2        2    42    38.0       4    27    9.0     11       5   \n",
      "76047  32338    1        1    37    30.0      10    31   14.0     14       2   \n",
      "76048  27127    1        1    23    23.0      13    37   12.0     14       2   \n",
      "76049  28789    2        1    31    24.0      10    31   17.0     16       2   \n",
      "\n",
      "       ...  onehotencoder__x0_P  onehotencoder__x0_S  onehotencoder__x0_U  \\\n",
      "0      ...                  0.0                  0.0                  0.0   \n",
      "1      ...                  0.0                  0.0                  0.0   \n",
      "2      ...                  0.0                  0.0                  0.0   \n",
      "3      ...                  0.0                  0.0                  0.0   \n",
      "4      ...                  0.0                  0.0                  0.0   \n",
      "...    ...                  ...                  ...                  ...   \n",
      "76045  ...                  0.0                  0.0                  0.0   \n",
      "76046  ...                  0.0                  0.0                  0.0   \n",
      "76047  ...                  0.0                  0.0                  0.0   \n",
      "76048  ...                  0.0                  0.0                  0.0   \n",
      "76049  ...                  0.0                  0.0                  0.0   \n",
      "\n",
      "       onehotencoder__x1_C  onehotencoder__x1_M  onehotencoder__x1_N  \\\n",
      "0                      0.0                  0.0                  1.0   \n",
      "1                      0.0                  0.0                  1.0   \n",
      "2                      0.0                  0.0                  1.0   \n",
      "3                      0.0                  0.0                  1.0   \n",
      "4                      0.0                  0.0                  1.0   \n",
      "...                    ...                  ...                  ...   \n",
      "76045                  0.0                  0.0                  1.0   \n",
      "76046                  0.0                  0.0                  1.0   \n",
      "76047                  0.0                  0.0                  1.0   \n",
      "76048                  0.0                  0.0                  1.0   \n",
      "76049                  0.0                  0.0                  1.0   \n",
      "\n",
      "       onehotencoder__x1_O  onehotencoder__x1_P  onehotencoder__x1_S  \\\n",
      "0                      0.0                  0.0                  0.0   \n",
      "1                      0.0                  0.0                  0.0   \n",
      "2                      0.0                  0.0                  0.0   \n",
      "3                      0.0                  0.0                  0.0   \n",
      "4                      0.0                  0.0                  0.0   \n",
      "...                    ...                  ...                  ...   \n",
      "76045                  0.0                  0.0                  0.0   \n",
      "76046                  0.0                  0.0                  0.0   \n",
      "76047                  0.0                  0.0                  0.0   \n",
      "76048                  0.0                  0.0                  0.0   \n",
      "76049                  0.0                  0.0                  0.0   \n",
      "\n",
      "       onehotencoder__x1_U  \n",
      "0                      0.0  \n",
      "1                      0.0  \n",
      "2                      0.0  \n",
      "3                      0.0  \n",
      "4                      0.0  \n",
      "...                    ...  \n",
      "76045                  0.0  \n",
      "76046                  0.0  \n",
      "76047                  0.0  \n",
      "76048                  0.0  \n",
      "76049                  0.0  \n",
      "\n",
      "[76050 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "#@checking for missing values  \n",
    "missing_counts_pre = []\n",
    "for i in X_train_ohe.columns:\n",
    "    #@checking for counts\n",
    "    missing_counts_pre.append([i, int(np.sum(X_train_ohe[i].isna()))])\n",
    "    #@printing the couting result as pd dataframe for missing_counts-variabe_name,num_of_missing_values\n",
    "missing_counts = pd.DataFrame(missing_counts_pre, columns = ['variable_name','num_of_missing_values'])\n",
    "print(missing_counts)\n",
    "#@below we are imputing for missing values by mean and storing the revised dataframe as X_train_ohe_imputed\n",
    "X_train_ohe_imputed = X_train_ohe.fillna(X_train_ohe.mean())\n",
    "print(X_train_ohe_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6: \n",
    "* Given a X_train_ohe_imputed (Pandas dataframe from Task 5) where all the categorical variables are already replaced with numeric values, print a list of top 20 highly correlated variables with respect to the target variable, and save the result as a Pandas dataframe named top20_df with 2 columns {variable,corr_score}. \n",
    "* Here, the corr_score between a variable x and the target variable y needs to be computed using the Pearson Correlation Coefficient (PCC). Please note, PCC ranges between -1 to +1. PCC score 0 means no correlation, while value towards +1 and -1 represent positive and negative correlations respectively. For instance, PCC=0.8 and PCC=-0.8 tell similar strength positive and negative correlations between the two subject variables.\n",
    "* Please do not include BWEIGHT in the top20_df list of top 20 correlated variable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable  corr_score\n",
      "0     WEEKS    0.565254\n",
      "1    GAINED    0.174466\n",
      "2    VISITS    0.129247\n",
      "3   HYPERPR    0.111375\n",
      "4   MARITAL    0.106297\n",
      "5       SEX    0.091260\n",
      "6    CIGNUM    0.086711\n",
      "7   RACEDAD    0.084284\n",
      "8   RACEMOM    0.078811\n",
      "9   PRETERM    0.075237\n",
      "10   CERVIX    0.068751\n",
      "11     MAGE    0.068550\n",
      "12  PINFANT    0.065963\n",
      "13   ECLAMP    0.065429\n",
      "14    MEDUC    0.055249\n",
      "15     FAGE    0.051981\n",
      "16    FEDUC    0.051537\n",
      "17   HYDRAM    0.050019\n",
      "18  HYPERCH    0.047094\n",
      "19  UTERINE    0.040339\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "#@corr_score between x and y needs to be computed as follows using PCC\n",
    "o = abs(X_train_ohe_imputed.corrwith(y_train, method='pearson'))\n",
    "top20 = o.sort_values(ascending=False).head(20)\n",
    "top20_df = pd.DataFrame(top20, columns=['corr_score'])\n",
    "'''print(top20_df)'''\n",
    "top20_df.reset_index(level=0, inplace=True)\n",
    "#@saving result as pandas dataframe named as top20_df above and including it with 2 columns{variable,corr_score}\n",
    "top20_df.columns = ['variable','corr_score']\n",
    "print(top20_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7: \n",
    "Given the X_train_ohe_imputed (as Pandas dataframe from task 5) and and top20_df (as Pandas Dataframe from Task 6) having 2 columns {variable_name,corr_score} similar to the one you computed in Task 6:\n",
    "* Please save as X_train_t20 keeping only the columns listed in the top20_df dataframe.\n",
    "* Repeat the process for X_test_ohe (obtained from task 4), and save it as X_test_t20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       WEEKS  GAINED  VISITS  HYPERPR  MARITAL  SEX  CIGNUM  RACEDAD  RACEMOM  \\\n",
      "0       38.0    50.0      16        0        1    1     0.0        1        1   \n",
      "1       41.0    50.0      13        0        2    2     0.0        2        2   \n",
      "2       40.0    38.0      11        1        1    1     0.0        1        1   \n",
      "3       40.0    20.0      10        0        2    1     0.0        1        1   \n",
      "4       39.0    30.0      13        0        1    2     0.0        1        1   \n",
      "...      ...     ...     ...      ...      ...  ...     ...      ...      ...   \n",
      "76045   40.0    51.0      18        0        2    1     0.0        1        1   \n",
      "76046   40.0    38.0       4        0        2    2    20.0        1        1   \n",
      "76047   39.0    30.0      10        0        1    1     0.0        1        1   \n",
      "76048   38.0    23.0      13        0        1    1     0.0        1        1   \n",
      "76049   39.0    24.0      10        0        1    2     0.0        1        1   \n",
      "\n",
      "       PRETERM  CERVIX  MAGE  PINFANT  ECLAMP  MEDUC  FAGE  FEDUC  HYDRAM  \\\n",
      "0            0       0    32        1       0     13    36   12.0     0.0   \n",
      "1            0       0    20        0       0     15    23   12.0     0.0   \n",
      "2            0       0    21        0       0     13    26   12.0     0.0   \n",
      "3            0       0    19        0       0     12    22   12.0     0.0   \n",
      "4            0       0    30        0       0     17    35   16.0     0.0   \n",
      "...        ...     ...   ...      ...     ...    ...   ...    ...     ...   \n",
      "76045        0       0    18        0       0     12    19   12.0     0.0   \n",
      "76046        0       0    27        0       0     11    42    9.0     0.0   \n",
      "76047        0       0    31        0       0     14    37   14.0     0.0   \n",
      "76048        0       0    37        0       0     14    23   12.0     0.0   \n",
      "76049        0       0    31        0       0     16    31   17.0     0.0   \n",
      "\n",
      "       HYPERCH  UTERINE  \n",
      "0            0        0  \n",
      "1            0        0  \n",
      "2            0        0  \n",
      "3            0        0  \n",
      "4            0        0  \n",
      "...        ...      ...  \n",
      "76045        0        0  \n",
      "76046        0        0  \n",
      "76047        0        0  \n",
      "76048        0        0  \n",
      "76049        0        0  \n",
      "\n",
      "[76050 rows x 20 columns]\n",
      "       WEEKS  GAINED  VISITS  HYPERPR  MARITAL  SEX  CIGNUM  RACEDAD  RACEMOM  \\\n",
      "0       39.0    33.0      16        0        1    2     0.0        1        1   \n",
      "1       39.0    20.0      10        0        1    2     0.0        2        2   \n",
      "2       38.0    34.0      12        0        2    1     0.0        2        2   \n",
      "3       39.0    16.0      15        0        1    2     0.0        1        1   \n",
      "4       39.0    30.0       7        0        1    2     0.0        1        1   \n",
      "...      ...     ...     ...      ...      ...  ...     ...      ...      ...   \n",
      "25345   40.0     0.0       8        0        1    2     0.0        1        1   \n",
      "25346   39.0    30.0      12        0        1    2     0.0        1        1   \n",
      "25347   41.0    28.0       3        0        1    2     0.0        1        1   \n",
      "25348   36.0    29.0      10        0        1    1     0.0        1        8   \n",
      "25349   38.0    30.0      12        0        1    1     0.0        1        1   \n",
      "\n",
      "       PRETERM  CERVIX  MAGE  PINFANT  ECLAMP  MEDUC  FAGE  FEDUC  HYDRAM  \\\n",
      "0            0       0    29        0       0     17    30   16.0     0.0   \n",
      "1            0       0    22        0       0     15    23   12.0     0.0   \n",
      "2            0       0    21        0       0     14    24   10.0     0.0   \n",
      "3            0       0    28        0       0     13    32   12.0     0.0   \n",
      "4            0       0    30        0       0     16    26   16.0     0.0   \n",
      "...        ...     ...   ...      ...     ...    ...   ...    ...     ...   \n",
      "25345        0       0    27        0       0     14    26   13.0     0.0   \n",
      "25346        0       0    29        0       0      6    33    8.0     0.0   \n",
      "25347        0       0    34        0       0     10    31   10.0     0.0   \n",
      "25348        0       0    39        0       0     17    38   17.0     0.0   \n",
      "25349        0       0    37        0       0     17    37   16.0     0.0   \n",
      "\n",
      "       HYPERCH  UTERINE  \n",
      "0            0        0  \n",
      "1            1        0  \n",
      "2            0        0  \n",
      "3            0        0  \n",
      "4            0        0  \n",
      "...        ...      ...  \n",
      "25345        0        0  \n",
      "25346        0        0  \n",
      "25347        0        0  \n",
      "25348        0        0  \n",
      "25349        0        0  \n",
      "\n",
      "[25350 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@TODO: Your code goes here\n",
    "#@saving the X_train_ohe_imputed(task5) and top20_df(task6) in to X_train_t20\n",
    "X_train_t20 = X_train_ohe_imputed[[i for i in top20_df['variable']]]\n",
    "print(X_train_t20)\n",
    "#@repeating process for X_test_ohe(task4) and storing/saving results in X_test_t20\n",
    "X_test_t20 = X_test_ohe[[i for i in top20_df['variable']]]\n",
    "print(X_test_t20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8: \n",
    "* Apply min-max scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_mm.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_mm.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74074074 0.51020408 0.32653061 ... 0.         0.         0.        ]\n",
      " [0.85185185 0.51020408 0.26530612 ... 0.         0.         0.        ]\n",
      " [0.81481481 0.3877551  0.2244898  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.77777778 0.30612245 0.20408163 ... 0.         0.         0.        ]\n",
      " [0.74074074 0.23469388 0.26530612 ... 0.         0.         0.        ]\n",
      " [0.77777778 0.24489796 0.20408163 ... 0.         0.         0.        ]]\n",
      "[[0.77777778 0.33673469 0.32653061 ... 0.         0.         0.        ]\n",
      " [0.77777778 0.20408163 0.20408163 ... 0.         1.         0.        ]\n",
      " [0.74074074 0.34693878 0.24489796 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.85185185 0.28571429 0.06122449 ... 0.         0.         0.        ]\n",
      " [0.66666667 0.29591837 0.20408163 ... 0.         0.         0.        ]\n",
      " [0.74074074 0.30612245 0.24489796 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@TODO: Your code goes here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#@Applying the min-max scaling on training dataset-X_train_t20 achieved from the Task 7 and savng results in X_train_scaled_mm.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_t20)\n",
    "X_train_scaled_mm = scaler.transform(X_train_t20)\n",
    "print(X_train_scaled_mm)\n",
    "#@likewsie will do scaling for X_test_t20 obtained from task 7 to be passed to X_test_scaled_mm\n",
    "X_test_scaled_mm = scaler.transform(X_test_t20)\n",
    "print(X_test_scaled_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9: \n",
    "* Apply standardization (i.e., normalization) scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_std.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_std.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29311281  1.45317334  0.95158568 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.90339477  1.45317334  0.14891977 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.50455891  0.5703685  -0.38619084 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " ...\n",
      " [ 0.10572305 -0.01816806 -0.65374615 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [-0.29311281 -0.53313756  0.14891977 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.10572305 -0.45957049 -0.65374615 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]]\n",
      "[[ 0.10572305  0.20253315  0.95158568 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.10572305 -0.75383877 -0.65374615 ... -0.12308901  8.45154255\n",
      "  -0.0582307 ]\n",
      " [-0.29311281  0.27610022 -0.11863554 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " ...\n",
      " [ 0.90339477 -0.16530221 -2.52663328 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [-1.09078454 -0.09173513 -0.65374615 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [-0.29311281 -0.01816806 -0.11863554 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#@applying standardization on training daset(X_train_t20-task7) and saving result in X_train_scaled_std\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_t20)\n",
    "X_train_scaled_std = scaler.transform(X_train_t20)\n",
    "print(X_train_scaled_std)\n",
    "#@likewise scaling for test datset -X_test_t20 obtained-Task 7 and saving the result as X_test_scaled_std\n",
    "X_test_scaled_std = scaler.transform(X_test_t20)\n",
    "print(X_test_scaled_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 10: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_closed_form_training** : It fits a linear regression model using the closed-form solution to obtain the coefficients, beta's, as a numpy array of m+1 values (Please recall class lecture), where *m* is the number of variables kept in X_train (the first argument to the function). Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_closed_form_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "* **RMSE**: It takes two lists: y_test, y_pred, where the first list represents ground truth (i.e., actual) target values for the given samples, and the second\n",
    "\n",
    "list represents a corresponding predicted values for exactly same number of samples in y_test. Compute and return the Root Mean Squared Error (RMSE) of the prediction. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_closed_form_training() function providing X_train_scaled_std, y_train obtained from Task 9, and save the returned results as betas_closed_form,cpu_time_closed_form.\n",
    "* Print betas_closed_form, cpu_time_closed_form\n",
    "* Call linear_regression_closed_form_predict() function providing betas_closed_form,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_closed_form.\n",
    "* Print rmse_closed_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#@Defined the linear regression function\n",
    "def linear_regression_closed_form_training(X_train,y_train):\n",
    "    from numpy.linalg import inv\n",
    "    from time import perf_counter\n",
    "    cpu_time = 0\n",
    "    betas=[]\n",
    "    t1_start=perf_counter()\n",
    "    #@TODO: Your code goes here\n",
    "#@ Appending a column of ones to X_train for intercept calculation and for getting m+1 values in beta\n",
    "    X_b=np.concatenate([X_train,np.ones((X_train.shape[0],1))],axis=1)\n",
    "#@formulating beta from closed form equation\n",
    "    betas = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)\n",
    "    t1_stop=perf_counter()\n",
    "    cpu_time=t1_stop-t1_start\n",
    "    return betas,cpu_time\n",
    "\n",
    "def linear_regression_closed_form_predict(betas, X_test):\n",
    "    y_pred = []\n",
    "    #@TODO: Your code goes here\n",
    "    #@ Again appending a column of ones to X_test for inserting intercept and for getting m+1 values in beta\n",
    "    transform_test=np.concatenate([X_test,np.ones((X_test.shape[0],1))],axis=1)\n",
    "    #@Getting y_prediction\n",
    "    y_pred=transform_test@betas\n",
    "    return y_pred\n",
    "\n",
    "def RMSE(y_test, y_pred):\n",
    "    #@TODO: Your code goes here\n",
    "    #@Getting RMSE\n",
    "    error=y_test-y_pred\n",
    "    return np.sqrt(error.ravel().dot(error.ravel())/len(error))\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71519322]\n",
      " [ 0.16451626]\n",
      " [ 0.03403515]\n",
      " [-0.08264272]\n",
      " [-0.07634788]\n",
      " [-0.12961155]\n",
      " [-0.09253333]\n",
      " [-0.05847441]\n",
      " [-0.04475425]\n",
      " [-0.03880411]\n",
      " [-0.02739149]\n",
      " [ 0.10283456]\n",
      " [ 0.0723325 ]\n",
      " [-0.03912292]\n",
      " [-0.01662473]\n",
      " [ 0.00921883]\n",
      " [-0.01519333]\n",
      " [-0.04398384]\n",
      " [-0.02638443]\n",
      " [-0.01920356]\n",
      " [ 7.25699786]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "from time import perf_counter\n",
    "# starting the stopwatch-counter\n",
    "betas_closed,cpu_time_closed = linear_regression_closed_form_training(X_train_scaled_std,np.array(y_train).reshape(-1,1))\n",
    "print(betas_closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01413979999998105\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "print(cpu_time_closed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0435247300214143\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "def RMSE(y_test,y_pred):\n",
    "    error=y_test-y_pred\n",
    "    return np.sqrt(error.ravel().dot(error.ravel())/len(error))\n",
    "y_pred = linear_regression_closed_form_predict(betas_closed,X_test_scaled_std)\n",
    "rmse_closed_form = RMSE(np.array(y_test).ravel(),y_pred.ravel())\n",
    "print(rmse_closed_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 11: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_batch_training** : It fits a linear regression model using the batch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate) and nEpoch (number of epochs) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_batch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_batch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=1000, and save the returned results as betas_batch,cpu_time_batch.\n",
    "* Print betas_batch, cpu_time_batch\n",
    "* Call linear_regression_gd_batch_predict() function providing betas_batch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_batch.\n",
    "* Print rmse_batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import random\n",
    "random.seed(554433)\n",
    "\n",
    "#@following is the function,with respect to betas to compute gradient of error\n",
    "def gradient(X, y, betas):\n",
    "    h = linear_regression_gd_batch_predict(betas, X)\n",
    "    #@ if np.isinf(h).any()\n",
    "    grad = np.dot(X.transpose(), (h - y))\n",
    "    return grad\n",
    "\n",
    "#@ following is the function to compute error in the current values of betas\n",
    "def cost(X, y , betas):\n",
    "    h = linear_regression_gd_batch_predict(betas, X)\n",
    "    J = np.dot((h-y).transpose(), (h-y))\n",
    "    J /= 2\n",
    "    return J[0]\n",
    "\n",
    "def linear_regression_gd_batch_training(X,y, alpha=0.01, nEpoch=1000):\n",
    "   \n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    #@TODO: Your code goes here\n",
    "    X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    error_list = []\n",
    "    iteration=0\n",
    "    t1_start = perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        for i in range(X.shape[0]):\n",
    "            X_val,y_val = X[i],y[i]\n",
    "            betas = betas - alpha * gradient(np.array([X_val]),np.array([y_val]), betas)\n",
    "            #@if np.linalg.norm(betas,np.inf)>1e12:\n",
    "            #@ break\n",
    "            #@iteration+=1\n",
    "            #@if iteration>nIterations:\n",
    "              #@  break\n",
    "        #@ for divergence break is used\n",
    "        #@if np.linalg.norm(betas,np.inf)>1e12:\n",
    "           #@ break\n",
    "        t1_stop = perf_counter()\n",
    "        cpu_time = t1_stop-t1_start\n",
    "        return betas, cpu_time\n",
    "                \n",
    "def linear_regression_gd_batch_predict(betas,X):\n",
    "    #@y_pred = []\n",
    "    #@TODO: Your code goes here\n",
    "    if X.shape[1]!=betas.shape[0]:\n",
    "        X = np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    y_pred = np.dot(X, betas)\n",
    "    return y_pred\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.75975538e+74]\n",
      " [ 2.03539903e+74]\n",
      " [-1.68754726e+74]\n",
      " [ 1.28447616e+74]\n",
      " [ 2.68528336e+74]\n",
      " [-3.05169663e+74]\n",
      " [-4.74126584e+73]\n",
      " [ 1.72758283e+74]\n",
      " [ 1.76642853e+74]\n",
      " [ 8.68942223e+73]\n",
      " [ 3.88047454e+73]\n",
      " [-8.77067531e+74]\n",
      " [-1.29874207e+73]\n",
      " [ 2.40373723e+74]\n",
      " [-4.98235044e+75]\n",
      " [-3.88914975e+74]\n",
      " [-5.23433124e+73]\n",
      " [ 7.11852674e+73]\n",
      " [ 2.35798233e+73]\n",
      " [ 9.35000972e+73]\n",
      " [-5.40537576e+75]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "#@starting the stopwatch \n",
    "betas_batch, cpu_time_batch = linear_regression_gd_batch_training(X_train_scaled_std, np.array(y_train).reshape(-1,1),alpha=0.01,nEpoch=1000)\n",
    "print(betas_batch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142829999999947\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "print(cpu_time_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.809958275866489e+75\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "def RMSE(y_test,y_pred):\n",
    "    error= y_test-y_pred\n",
    "    return np.sqrt(error.dot(error)/len(error))\n",
    "y_pred = linear_regression_gd_batch_predict(betas_batch,X_test_scaled_std)\n",
    "rmse_batch = RMSE(np.array(y_test).ravel(),y_pred.ravel())\n",
    "print(rmse_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 12:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_stochastic_training** : It fits a linear regression model using the stochastic gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_stochastic_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_stochastic_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=50, nIteration=100 , and save the returned results as betas_stochastic,cpu_time_stochastic.\n",
    "* Print betas_stochastic, cpu_time_stochastic\n",
    "* Call linear_regression_gd_stochastic_predict() function providing betas_stochastic,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_stochastic.\n",
    "* Print rmse_stochastic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import random\n",
    "random.seed(554433)\n",
    "\n",
    "#@following is the function,with respect to betas to compute gradient of error\n",
    "def gradient(X, y, betas):\n",
    "    h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "    #@ if np.isinf(h).any()\n",
    "    grad = np.dot(X.transpose(), (h - y))\n",
    "    return grad\n",
    "\n",
    "#@ following is the function to compute error in the current values of betas\n",
    "def cost(X, y , betas):\n",
    "    h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "    J = np.dot((h-y).transpose(), (h-y))\n",
    "    J /= 2\n",
    "    return J[0]\n",
    "\n",
    "\n",
    "def linear_regression_gd_stochastic_training(X,y,alpha=0.01,batch_size = 32, nEpoch=50,nIterations=100):\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "\n",
    "    ## YOUR CODE HERE ###\n",
    "    X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    error_list = []\n",
    "    iteration=0\n",
    "    t1_start = perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        for i in range(X.shape[0]):\n",
    "            X_val,y_val = X[i],y[i]\n",
    "            betas = betas - alpha * gradient(np.array([X_val]),np.array([y_val]), betas)\n",
    "            if np.linalg.norm(betas,np.inf)>1e12:\n",
    "                break\n",
    "            iteration+=1\n",
    "            if iteration>nIterations:\n",
    "                break\n",
    "        #@ for divergence break is used\n",
    "        if np.linalg.norm(betas,np.inf)>1e12:\n",
    "            break\n",
    "        t1_stop = perf_counter()\n",
    "        cpu_time = t1_stop-t1_start\n",
    "        return betas, cpu_time\n",
    "    \n",
    "#@using stochastic gradient descent for linear regression.It includes function in order to compute the predictions.\n",
    "\n",
    "def linear_regression_gd_stochastic_predict(betas, X):\n",
    "\n",
    "    y_pred = []\n",
    "    ## YOUR CODE HERE ###\n",
    "    if X.shape[1]!=betas.shape[0]:\n",
    "        X = np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "          \n",
    "    y_pred = np.dot(X, betas)\n",
    "    return y_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.56793934]\n",
      " [ 0.25743653]\n",
      " [ 0.53530479]\n",
      " [ 0.17888193]\n",
      " [ 0.17818434]\n",
      " [-0.06123982]\n",
      " [ 0.29556357]\n",
      " [-0.66850014]\n",
      " [-0.04804292]\n",
      " [ 0.10101505]\n",
      " [-0.43130318]\n",
      " [-0.28310641]\n",
      " [-0.15608059]\n",
      " [-0.23699589]\n",
      " [-0.2776839 ]\n",
      " [ 0.11172276]\n",
      " [-0.35616141]\n",
      " [-0.20594723]\n",
      " [ 0.12140823]\n",
      " [ 0.4403228 ]\n",
      " [-0.26599431]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "#@starting the stopwatch \n",
    "betas_stochastic, cpu_time_stochastic = linear_regression_gd_stochastic_training(X_train_scaled_std, np.array(y_train).reshape(-1,1),alpha=0.01)\n",
    "\n",
    "print(betas_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003618899999992209\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "print(cpu_time_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1932074728838513\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "def RMSE(y_test,y_pred):\n",
    "    error= y_test-y_pred\n",
    "    return np.sqrt(error.dot(error)/len(error))\n",
    "y_pred = linear_regression_gd_stochastic_predict(betas_stochastic,X_test_scaled_std)\n",
    "rmse_stochastic = RMSE(np.array(y_test).ravel(),y_pred.ravel())\n",
    "print(rmse_stochastic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_minibatch_training** : It fits a linear regression model using the minibatch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations), and batch_size parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_minibatch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_minibatch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.001,nEpoch=50, nIteration=1000,batch_size=32, and save the returned results as betas_minibatch,cpu_time_minibatch.\n",
    "* Print betas_minibatch, cpu_time_minibatch\n",
    "* Call linear_regression_gd_minibatch_predict() function providing betas_minibatch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_minibatch.\n",
    "* Print rmse_minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "#@following is the function,with respect to betas to compute gradient of error\n",
    "def gradient(X, y, betas):\n",
    "    h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "    #@ if np.isinf(h).any()\n",
    "    grad = np.dot(X.transpose(), (h - y))\n",
    "    return grad\n",
    "\n",
    "#@ following is the function to compute error in the current values of betas\n",
    "def cost(X, y , betas):\n",
    "    h = linear_regression_gd_minibatch_predict(betas, X)\n",
    "    J = np.dot((h-y).transpose(), (h-y))\n",
    "    J /= 2\n",
    "    return J[0]\n",
    "#function for creating a list which contains mini batches\n",
    "def create_mini_batches(X,y,batch_size):\n",
    "    mini_batches=[]\n",
    "    data=np.hstack((X,y))\n",
    "    np.random.shuffle(data)\n",
    "    n_minibatches=data.shape[0] // batch_size\n",
    "    i=0\n",
    "    \n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[i * batch_size:data.shape[0]]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    return mini_batches\n",
    "\n",
    "#@below is the functionfor performing mini batch gradient descent.\n",
    "def linear_regression_gd_minibatch_training(X,y,alpha=0.001,nEpoch=50, nIteration=1000, batch_size=32):\n",
    "    import random\n",
    "    from time import perf_counter\n",
    "    import numpy as np\n",
    "    random.seed(554433)\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "\n",
    "    ## YOUR CODE HERE ###\n",
    "    X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "#@print(X.shape)\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    iteration=0\n",
    "    t1_start = perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        if iteration>nIteration:\n",
    "                break\n",
    "        mini_batches = create_mini_batches(X,y,batch_size) \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini,y_mini = mini_batch\n",
    "            betas = betas - alpha * gradient(X_mini,y_mini, betas)\n",
    "            #@Error_list.append(cost(X_mini,y_mini,betas))\n",
    "            iteration+=1\n",
    "            if iteration>nIteration:\n",
    "                break\n",
    "        t1_stop = perf_counter()\n",
    "        cpu_time = t1_stop-t1_start\n",
    "        return betas, cpu_time    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return betas,cpu_time\n",
    "#@Linear regression using mini batch gradient descent and accordingly function to compute predictons\n",
    "def linear_regression_gd_minibatch_predict(betas,X):\n",
    "    y_pred = []\n",
    "    ## YOUR CODE HERE ###\n",
    "    if X.shape[1]!=betas.shape[0]:\n",
    "        X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "        y_pred = np.dot(X,betas)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.25030856e+00]\n",
      " [ 7.21102160e-01]\n",
      " [ 1.65084822e-01]\n",
      " [ 7.56771493e-02]\n",
      " [-4.79188690e-02]\n",
      " [-8.76888480e-02]\n",
      " [-1.21309884e-01]\n",
      " [-9.39665790e-02]\n",
      " [-5.61220891e-02]\n",
      " [-6.15720349e-02]\n",
      " [-7.18586159e-02]\n",
      " [-3.39604558e-02]\n",
      " [ 7.98587062e-02]\n",
      " [ 9.65963374e-02]\n",
      " [-9.26131655e-02]\n",
      " [-6.41061911e-03]\n",
      " [ 4.48301157e-02]\n",
      " [-2.51075827e-03]\n",
      " [-6.06274575e-02]\n",
      " [-4.09458126e-02]\n",
      " [-5.24396156e-02]]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "#@starting the stopwatch\n",
    "betas_minibatch,cpu_time_minibatch = linear_regression_gd_minibatch_training(X_train_scaled_std,np.array(y_train).reshape(-1,1),alpha=0.001,batch_size=32,nEpoch=50,nIteration=1000)\n",
    "print(betas_minibatch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15562690000001567\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "print(cpu_time_minibatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0489667830241165\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "def RMSE(y_test,y_pred):\n",
    "  error = y_test-y_pred\n",
    "  return np.sqrt(error.dot(error)/len(error))\n",
    "y_pred = linear_regression_gd_minibatch_predict(betas_minibatch,X_test_scaled_std)\n",
    "rmse_minibatch = RMSE(np.array(y_test).ravel(),y_pred.ravel())\n",
    "print(rmse_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14:\n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, print a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the best predictive performance in terms of RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictive performance in terms of RMSE:  1.0435247300214143\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "#@ sets of results from {closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD}\n",
    "b=(1.0435247300214143,7.809958275866489e+75,3.1932074728838513\n",
    ",1.0475600459968117)\n",
    "if type(b[0])==str:\n",
    "    b=tuple(map(int, b))\n",
    "    \n",
    "#@find minimum time using min function\n",
    "print('Best predictive performance in terms of RMSE: ',min(b))\n",
    "#@ After generating the output,we can see that the best predcitive performance in terms of RMSE is Closed-form.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15: \n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, print a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the least training cpu time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least training cpu time:  0.002703299999666342\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "#@ 4 sets of results from the 4 experiments are {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"}\n",
    "a=(0.021475900000041293,0.6586885999995502,0.002703299999666342,0.14441580000038812)\n",
    "#@ if the given result is in string format then convert it into int format\n",
    "if type(a[0])==str:\n",
    "    a=tuple(map(int, a))\n",
    "    \n",
    "#@find minimum time using min function\n",
    "print('Least training cpu time: ',min(a))\n",
    "#@After generating the output for least cpu training time is for stochastic-GD\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 16: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively, \n",
    "* call your implementation of Task 12: stochastic gradient descent based linear regression for each of these learning rates: {0.0001, 0.001, 0.05, 0.01, 0.1, 1.0}\n",
    "    * Please use the nIteration (number of iterations), nEpoch (number of epoch) parameters in your implementation of the gradient descent algorithm.\n",
    "\n",
    "* For each of the linear regression model, using the computed beta values, predict the test samples provided in the \"X_test_scaled_std\" argument, and let's name your prediction \"y_pred\".\n",
    "* Compute Root Mean Squared Error (RMSE) of your prediction using the RMSE() function you defined in Task 10.\n",
    "* Finally, print the learning rate that shows the best test performance, and also print as a pandas dataframe named summary with 2 columns: {learning_rate, test_RMSE} containing RMSE's of the 6 linear regression models. Also, print the best performing learning rate.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  test_RMSE\n",
      "learning_rate              \n",
      "0.0001         6.672902e+00\n",
      "0.0010         2.886759e+00\n",
      "0.0500         4.925101e+12\n",
      "0.0100         6.809476e+00\n",
      "0.1000         2.536329e+12\n",
      "1.0000         1.351940e+13\n",
      "\n",
      "The best performance learning rate is 0.001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import random\n",
    "random.seed(554433)\n",
    "\n",
    "# function to compute gradient of error function with regards to betas\n",
    "def gradient(X, y, betas):\n",
    "  h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "  # if np.isinf(h).any()\n",
    "  grad = np.dot(X.transpose(), (h - y))\n",
    "  return grad\n",
    "\n",
    "# function to compute the error for current values of betas\n",
    "def cost(X, y, betas):\n",
    "  h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "  J = np.dot((h - y).transpose(), (h - y))\n",
    "  J /= 2\n",
    "  return J[0]\n",
    "\n",
    "def linear_regression_gd_stochastic_training(X,y,alpha=0.01,batch_size = 32, nEpoch=50,nIterations=100):\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    error_list = []\n",
    "    iteration=0\n",
    "    t1_start = perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        for i in range(X.shape[0]):\n",
    "            X_val,y_val = X[i],y[i]\n",
    "            betas = betas - alpha * gradient(np.array([X_val]),np.array([y_val]), betas)\n",
    "            if np.linalg.norm(betas,np.inf)>1e12:\n",
    "                break\n",
    "            iteration+=1\n",
    "            if iteration>nIterations:\n",
    "              \n",
    "                break\n",
    "        #@ for divergence break is used\n",
    "        if np.linalg.norm(betas,np.inf)>1e12:\n",
    "            break\n",
    "        t1_stop = perf_counter()\n",
    "        cpu_time = t1_stop-t1_start\n",
    " \n",
    "    return betas, cpu_time\n",
    "    \n",
    "def linear_regression_gd_stochastic_predict(betas, X):\n",
    "    y_pred = []\n",
    "    if X.shape[1]!=betas.shape[0]:\n",
    "        X =np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    y_pred = np.dot(X, betas)\n",
    "    return y_pred\n",
    " \n",
    "def RMSE(y_test,y_pred):\n",
    "  error = y_test-y_pred\n",
    "  return np.sqrt(error.dot(error)/len(error))\n",
    "#below is the best performing learning rate\n",
    "learning_rates = [0.0001,0.001,0.05,0.01,0.1,1]\n",
    "betalr = [linear_regression_gd_stochastic_training(X_train_scaled_std,np.array(y_train).reshape(-1,1),alpha=rate,nEpoch=50,nIterations=1000)[0] for rate in learning_rates]\n",
    "summary = {\"learning_rate\":learning_rates,\"test_RMSE\":[RMSE(np.array(y_test).ravel(),linear_regression_gd_stochastic_predict(betas,X_test_scaled_std).ravel()) for betas in betalr] }\n",
    "df = pd.DataFrame.from_dict(summary)\n",
    "result=df.set_index(\"learning_rate\")\n",
    "print(result)\n",
    "print(\"\\nThe best performance learning rate is\",list(result.idxmin())[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 17:\n",
    "* Utilizing the best trained linear regression model (so far), predict the target for each of the samples in the judge_dataset.\n",
    "    * I believe you will not forget to do the following before call in the prediction algorithm:\n",
    "        - Save the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "        - Perform onehot encoding using the same encoder you used to encode X_test (Task 4). \n",
    "        - keep only the same top 20 variables as you did in Task 7. \n",
    "        - scale the input variables based on the same metrics you used to scale the training dataset (Task 9). \n",
    "    * Now, call the prediction function of that model to obtain y_pred.\n",
    "    * Prepare and print as a pandas dataframe having columns: {ID, BWEIGHT}, where ID will the ID of the judge sample, and BWEIGHT is the corresponding y_pred value from your model prediction.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID   BWEIGHT\n",
      "0        1  7.159180\n",
      "1        2  7.521792\n",
      "2        3  6.504583\n",
      "3        4  7.163844\n",
      "4        5  6.768008\n",
      "...    ...       ...\n",
      "1995  1996  5.884734\n",
      "1996  1997  7.386190\n",
      "1997  1998  7.859144\n",
      "1998  1999  7.391354\n",
      "1999  2000  5.725940\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "#@Saving the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "judge_dataset = pd.read_csv('dataset/judge-without-labels.csv',delimiter=',')\n",
    "ID_judge=judge_dataset['ID']\n",
    "judge_dataset.drop(['ID'],axis=1, inplace=True)\n",
    "\n",
    "#@Performing onehot encoding using the same encoder you used to encode X_test (Task 4).\n",
    "judge_ohe = judge_dataset.drop(categorical_cols, axis= 1).reset_index()\n",
    "judge_ohe1 = pd.DataFrame(column_trans.fit_transform(judge_dataset),columns=column_trans.get_feature_names())\n",
    "judge_ohe = pd.concat([judge_ohe, judge_ohe1], axis=1)\n",
    "\n",
    "#@keep only the same top 20 variables as you did in Task 7\n",
    "#@X_train_t20 = X_train_ohe_imputed[[i for i in top20_df['variable']]]\n",
    "judge_t20 = judge_ohe[[i for i in top20_df['variable']]]\n",
    "\n",
    "#@scale the input variables based on the same metrics you used to scale the training dataset (Task 9).\n",
    "judge_scaled_std = scaler.transform(judge_t20)\n",
    "\n",
    "judge_pred = linear_regression_closed_form_predict(betas_closed, judge_scaled_std)\n",
    "o = np.concatenate((np.resize(ID_judge,[ID_judge.shape[0],1]),judge_pred), axis=1)\n",
    "\n",
    "output_df = pd.DataFrame(o, columns=['ID', 'BWEIGHT'])\n",
    "output_df['ID'] = output_df['ID'].astype(int)\n",
    "print(output_df)\n",
    "output_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graduate Students only\n",
    "## Task 18:\n",
    "Bring your best model and submit your solution at https://www.kaggle.com/c/birth-weight-prediction\n",
    "Submit multiple entries. Demonstrate your effort by pushing yourself to improve your own score or beat other submissions (if any available) until the deadline and document your scores and list what changes you made in your submitted solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  test_RMSE\n",
      "learning_rate              \n",
      "0.0001         6.672902e+00\n",
      "0.0010         2.886759e+00\n",
      "0.0500         4.925101e+12\n",
      "0.0100         6.809476e+00\n",
      "0.1000         2.536329e+12\n",
      "1.0000         1.351940e+13\n",
      "\n",
      "The best performance learning rate is 0.001\n",
      "Best predictive performance in terms of RMSE:  1.0435247300214143\n",
      "Least training cpu time:  0.002703299999666342\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "#@Multiple entries on Kaggle has been submitted,I scored 1.613 and I tried revising the one hot encoding \n",
    "#@and I considered betas_stochastic earlier and in the later submission I have taken betas_closed for Judge_label datasets,\n",
    "#@due to which the predicted BWEIGHT results were much better as compared in the previous version.So our results showcase\n",
    "#@that,the lowest RMSE is for Closed form where closed form is a Convex solution \n",
    "#@and it is at global minima.\n",
    "\n",
    "\n",
    "#@Lower RMSE values proposed a good fit in our assignment. \n",
    "#@So we understand that If the major objective of the model is prediction, RMSE is a reliable indicator \n",
    "#@of how accurately the model predicts the response, and it is the most important criterion for fit.\n",
    "\n",
    "\n",
    "#@Also understood by using the learning rate which is an accuracy of an instrument in an algorithm to be optimized \n",
    "#@that defines the step size at each iteration as the program moves toward a loss function minimum.\n",
    "#@Moreover, our model's learning rate influences how quickly it can converge to a local minima \n",
    "#@which also means arriving at the good accuracy.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import random\n",
    "random.seed(554433)\n",
    "\n",
    "# function to compute gradient of error function w.r.t. betas\n",
    "def gradient(X, y, betas):\n",
    "  h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "  # if np.isinf(h).any()\n",
    "  grad = np.dot(X.transpose(), (h - y))\n",
    "  return grad\n",
    "\n",
    "# function to compute the error for current values of betas\n",
    "def cost(X, y, betas):\n",
    "  h = linear_regression_gd_stochastic_predict(betas, X)\n",
    "  J = np.dot((h - y).transpose(), (h - y))\n",
    "  J /= 2\n",
    "  return J[0]\n",
    "\n",
    "def linear_regression_gd_stochastic_training(X,y,alpha=0.01,batch_size = 32, nEpoch=50,nIterations=100):\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    betas = np.zeros((X.shape[1],1))\n",
    "    error_list = []\n",
    "    iteration=0\n",
    "    t1_start = perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        for i in range(X.shape[0]):\n",
    "            X_val,y_val = X[i],y[i]\n",
    "            betas = betas - alpha * gradient(np.array([X_val]),np.array([y_val]), betas)\n",
    "            if np.linalg.norm(betas,np.inf)>1e12:\n",
    "                break\n",
    "            iteration+=1\n",
    "            if iteration>nIterations:\n",
    "              \n",
    "                break\n",
    "        #@ for divergence break is used\n",
    "        if np.linalg.norm(betas,np.inf)>1e12:\n",
    "            break\n",
    "        t1_stop = perf_counter()\n",
    "        cpu_time = t1_stop-t1_start\n",
    " \n",
    "    return betas, cpu_time\n",
    "    \n",
    "def linear_regression_gd_stochastic_predict(betas, X):\n",
    "    y_pred = []\n",
    "    if X.shape[1]!=betas.shape[0]:\n",
    "        X =np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "    y_pred = np.dot(X, betas)\n",
    "    return y_pred\n",
    "def RMSE(y_test,y_pred):\n",
    "  error = y_test-y_pred\n",
    "  return np.sqrt(error.dot(error)/len(error))\n",
    "#below is the best performing learning rate\n",
    "learning_rates = [0.0001,0.001,0.05,0.01,0.1,1]\n",
    "betalr = [linear_regression_gd_stochastic_training(X_train_scaled_std,np.array(y_train).reshape(-1,1),alpha=rate,nEpoch=50,nIterations=1000)[0] for rate in learning_rates]\n",
    "summary = {\"learning_rate\":learning_rates,\"test_RMSE\":[RMSE(np.array(y_test).ravel(),linear_regression_gd_stochastic_predict(betas,X_test_scaled_std).ravel()) for betas in betalr] }\n",
    "df = pd.DataFrame.from_dict(summary)\n",
    "result=df.set_index(\"learning_rate\")\n",
    "print(result)\n",
    "print(\"\\nThe best performance learning rate is\",list(result.idxmin())[0])\n",
    "\n",
    "## YOUR CODE HERE ###\n",
    "#@ sets of results from {closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD}\n",
    "b=(1.0435247300214143,7.809958275866489e+75,3.1932074728838513\n",
    ",1.0475600459968117)\n",
    "if type(b[0])==str:\n",
    "    b=tuple(map(int, b))\n",
    "    \n",
    "#@find minimum time using min function\n",
    "print('Best predictive performance in terms of RMSE: ',min(b))\n",
    "\n",
    "## YOUR CODE HERE ###\n",
    "#@ 4 sets of results from the 4 experiments are {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"}\n",
    "a=(0.021475900000041293,0.6586885999995502,0.002703299999666342,0.14441580000038812)\n",
    "#@ if the given result is in string format then convert it into int format\n",
    "if type(a[0])==str:\n",
    "    a=tuple(map(int, a))\n",
    "    \n",
    "#@find minimum time using min function\n",
    "print('Least training cpu time: ',min(a))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
